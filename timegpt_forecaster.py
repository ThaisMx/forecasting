import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import os
from dotenv import load_dotenv
import json
import logging
import traceback
import requests
import plotly.graph_objects as go
import time
from typing import Dict, List, Union, Tuple, Optional, Any
from dateutil.parser import parse

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

load_dotenv()

class TimeGPTForecaster:
    def __init__(self):
        """Inicializa o forecaster."""
        self.api_key = os.getenv('TIMEGPT_API_KEY')
        if not self.api_key:
            raise ValueError("TIMEGPT_API_KEY n√£o encontrada no arquivo .env")
            
        # URL base correta para a API da Nixtla TimeGPT
        self.base_url = "https://api.nixtla.io"
        self.headers = {
            "accept": "application/json",
            "content-type": "application/json",
            "Authorization": f"Bearer {self.api_key}"
        }

        # Modelos dispon√≠veis
        self.available_models = ["timegpt-1", "timegpt-1-long-horizon"]
        
        # Fun√ß√µes de perda dispon√≠veis para fine-tuning
        self.loss_functions = ["mae", "mse", "rmse", "mape", "smape"]

    def prepare_data(self, df, h=7, freq='D', level=None, model="timegpt-1", X_df=None):
        """
        Prepara os dados para a API do TimeGPT
        
        Args:
            df: DataFrame com colunas 'ds' e 'y'
            h: horizonte de previs√£o
            freq: frequ√™ncia dos dados ('D' para di√°rio, 'W' para semanal, etc.)
            level: n√≠veis para intervalos de previs√£o (ex: [80, 90])
            model: modelo a ser usado (ex: "timegpt-1", "timegpt-1-long-horizon")
            X_df: DataFrame com vari√°veis ex√≥genas (opcional)
        """
        logger.info(f"Preparando dados para previs√£o. Shape inicial: {df.shape}")
        
        try:
            # Converte datas para o formato correto
            df = df.copy()
            df['ds'] = pd.to_datetime(df['ds'])
            df['y'] = pd.to_numeric(df['y'], errors='coerce')
            
            # Remove valores inv√°lidos
            df = df.dropna()
            df = df[~np.isinf(df['y'].values)]
            
            # Agrupa por data e calcula a m√©dia para lidar com datas repetidas
            df = df.groupby('ds')['y'].mean().reset_index()
            
            if len(df) == 0:
                raise ValueError("Nenhum dado v√°lido ap√≥s limpeza")
            
            logger.info(f"Dados preparados. N√∫mero de pontos v√°lidos: {len(df)}")
            logger.info(f"Primeiros valores: {df['y'].head().tolist()}")
            logger.info(f"√öltimos valores: {df['y'].tail().tolist()}")
            
            # Formato correto baseado na documenta√ß√£o mais recente
            df_dict = {
                "unique_id": ["series1"] * len(df),
                "ds": df['ds'].dt.strftime('%Y-%m-%d').tolist(),
                "y": df['y'].tolist()
            }
            
            # Verificar se o modelo √© v√°lido
            if model not in self.available_models:
                logger.warning(f"Modelo {model} n√£o reconhecido. Usando o modelo padr√£o timegpt-1")
                model = "timegpt-1"
                
            # Criar o payload b√°sico
            payload = {
                "df": df_dict,
                "h": h,
                "freq": freq,
                "model": model
            }
            
            # Adicionar n√≠veis para intervalos de previs√£o se especificados
            if level:
                payload["level"] = level
            
            # Adicionar vari√°veis ex√≥genas se fornecidas
            if X_df is not None and not X_df.empty:
                logger.info("Processando vari√°veis ex√≥genas")
                X_df = X_df.copy()
                
                # Garantir que as datas sejam datetime
                X_df['ds'] = pd.to_datetime(X_df['ds'])
                
                # Converter para o formato esperado pela API
                exog_dict = {
                    "unique_id": ["series1"] * len(X_df),
                    "ds": X_df['ds'].dt.strftime('%Y-%m-%d').tolist()
                }
                
                # Adicionar cada coluna ex√≥gena
                for col in X_df.columns:
                    if col != 'ds':
                        exog_dict[col] = X_df[col].tolist()
                
                payload["X_df"] = exog_dict
                logger.info(f"Vari√°veis ex√≥genas inclu√≠das: {[col for col in X_df.columns if col != 'ds']}")
            
            return payload
        except Exception as e:
            logger.error(f"Erro ao preparar dados: {str(e)}")
            logger.error(f"Traceback: {traceback.format_exc()}")
            raise

    def get_forecast(self, df, h=7, freq='D', level=[80, 90], model="timegpt-1", X_df=None):
        """
        Obt√©m previs√£o do TimeGPT
        
        Args:
            df: DataFrame com colunas 'ds' e 'y'
            h: horizonte de previs√£o
            freq: frequ√™ncia dos dados ('D' para di√°rio, 'W' para semanal, etc.)
            level: n√≠veis para intervalos de previs√£o
            model: modelo a ser usado
            X_df: DataFrame com vari√°veis ex√≥genas (opcional)
        """
        try:
            params = self.prepare_data(df, h=h, freq=freq, level=level, model=model, X_df=X_df)
            logger.info(f"Enviando requisi√ß√£o para API do TimeGPT com horizonte de {h} {freq}")
            
            # Endpoint correto baseado na documenta√ß√£o mais recente
            endpoint = f"{self.base_url}/api/forecast"
            logger.info(f"Endpoint: {endpoint}")
            logger.info(f"Par√¢metros enviados: {json.dumps({k: v for k, v in params.items() if k != 'df'})}")
            
            # Fazer a chamada API
            response = requests.post(
                endpoint,
                headers=self.headers,
                json=params,
                timeout=60  # Aumentando o timeout para permitir requisi√ß√µes maiores
            )
            
            # Verificar resposta
            if response.status_code != 200:
                logger.error(f"Erro na API: {response.status_code} - {response.text}")
                
                # Tentar endpoint alternativo se o primeiro falhar com 404
                if response.status_code == 404:
                    logger.info("Tentando endpoint alternativo ap√≥s erro 404")
                    alt_endpoint = f"{self.base_url}/forecast"
                    
                    response = requests.post(
                        alt_endpoint,
                        headers=self.headers,
                        json=params,
                        timeout=60
                    )
                    
                    if response.status_code != 200:
                        logger.error(f"Erro tamb√©m no endpoint alternativo: {response.status_code} - {response.text}")
                        raise Exception(f"Erro na API: {response.status_code} - {response.text}")
                    else:
                        logger.info("Requisi√ß√£o bem-sucedida com endpoint alternativo")
                else:
                raise Exception(f"Erro na API: {response.status_code} - {response.text}")
            
            # Processar resposta
            result = response.json()
            logger.info(f"Resposta recebida com sucesso")
            
            if not result:
                logger.error("Resposta da API est√° vazia")
                raise Exception("Resposta da API est√° vazia")
            
            # Extrair os dados de previs√£o - verificar estrutura da resposta
            forecast_data = None
            using_new_format = False
            
            # Verificar se a resposta est√° no formato esperado
            if 'data' in result and len(result['data']) > 0:
                forecast_data = result['data'][0]
            elif 'forecast' in result and len(result['forecast']) > 0:
                forecast_data = result['forecast'][0]
            elif 'prediction' in result and len(result['prediction']) > 0:
                forecast_data = result['prediction'][0]
            # Novo formato de resposta com timestamp e value
            elif 'timestamp' in result and 'value' in result:
                logger.info("Detectado novo formato de resposta com timestamp e value")
                
                # Em vez de usar as datas retornadas pela API, vamos gerar datas continuando dos dados hist√≥ricos
                try:
                    last_date = pd.to_datetime(df['ds']).max()
                    logger.info(f"√öltima data nos dados hist√≥ricos: {last_date}")
                    
                    # Gerar datas futuras com base na frequ√™ncia
                    if freq == 'D':
                        future_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=h, freq='D')
                    elif freq == 'W':
                        future_dates = pd.date_range(start=last_date + pd.Timedelta(days=7), periods=h, freq='W')
                    elif freq == 'M':
                        future_dates = pd.date_range(start=last_date + pd.Timedelta(days=31), periods=h, freq='MS')
                    else:
                        future_dates = pd.date_range(start=last_date, periods=h + 1, freq=freq)[1:]
                    
                    logger.info(f"Datas de previs√£o geradas: de {future_dates[0]} a {future_dates[-1]}")
                    
                    # Limitar os valores de previs√£o ao mesmo tamanho das datas geradas
                    forecast_values = result['value'][:len(future_dates)]
                    if len(forecast_values) < len(future_dates):
                        logger.warning(f"N√∫mero de valores de previs√£o ({len(forecast_values)}) menor que o n√∫mero de datas geradas ({len(future_dates)})")
                        
                        # Verificar se o n√∫mero de valores de previs√£o corresponde ao n√∫mero de datas futuras
                        if len(forecast_values) > 1 and len(set(forecast_values)) > 1:
                            logger.info("Estendendo previs√£o com interpola√ß√£o baseada em tend√™ncia")
                            # Calcular tend√™ncia com regress√£o linear
                            x = np.arange(len(forecast_values))
                            y = np.array(forecast_values)
                            slope, intercept = np.polyfit(x, y, 1)
                            
                            # Estender os valores com a tend√™ncia calculada e adicionar alguma variabilidade
                            last_x = len(forecast_values) - 1
                            extension = []
                            for i in range(len(future_dates) - len(forecast_values)):
                                next_x = last_x + i + 1
                                # Valor da tend√™ncia + pequeno ru√≠do para naturalidade
                                pred_value = slope * next_x + intercept
                                noise = np.random.normal(0, max(0.01 * abs(pred_value), 0.1))
                                extension.append(pred_value + noise)
                            
                            forecast_values = forecast_values + extension
                        else:
                            # Se s√≥ temos valores id√™nticos ou apenas um, usar dados hist√≥ricos para estimar tend√™ncia
                            logger.info("Estendendo previs√£o usando tend√™ncia hist√≥rica")
                            
                            # Usar dados hist√≥ricos para estimar tend√™ncia se dispon√≠veis
                            if df is not None and len(df) > 3:
                                hist_values = df['y'].values
                                if len(hist_values) > 1:
                                    hist_x = np.arange(len(hist_values))
                                    hist_slope, hist_intercept = np.polyfit(hist_x, hist_values, 1)
                                    hist_std = np.std(hist_values)
                                    
                                    # Usar o √∫ltimo valor de previs√£o como ponto de partida
                                    start_value = forecast_values[-1] if forecast_values else hist_values[-1]
                                    
                                    # Gerar extens√£o com base na tend√™ncia hist√≥rica
                                    extension = []
                                    for i in range(len(future_dates) - len(forecast_values)):
                                        # Adicionar tend√™ncia + ru√≠do baseado na variabilidade hist√≥rica
                                        next_value = start_value + hist_slope * (i + 1)
                                        noise = np.random.normal(0, max(hist_std * 0.2, 0.01 * abs(start_value)))
                                        extension.append(next_value + noise)
                                    
                                    forecast_values = forecast_values + extension
                                else:
                                    # Poucos dados hist√≥ricos, usar o √∫ltimo valor + varia√ß√£o aleat√≥ria
                                    base_value = forecast_values[-1] if forecast_values else hist_values[-1]
                                    extension = [base_value * (1 + np.random.uniform(-0.05, 0.05)) 
                                                for _ in range(len(future_dates) - len(forecast_values))]
                                    forecast_values = forecast_values + extension
                            else:
                                # Sem dados hist√≥ricos ou previs√£o, repetir √∫ltimo valor com varia√ß√£o
                                logger.warning("Sem dados hist√≥ricos para basear a extens√£o da previs√£o")
                                base_value = forecast_values[-1] if forecast_values else 1.0  # Valor padr√£o
                                extension = [base_value * (1 + np.random.uniform(-0.05, 0.05)) 
                                            for _ in range(len(future_dates) - len(forecast_values))]
                                forecast_values = forecast_values + extension
                    
                    # Criar um dicion√°rio no formato que o resto do c√≥digo espera
                    forecast_data = {
                        'ds': [d.strftime('%Y-%m-%d') for d in future_dates],
                        'TimeGPT': forecast_values
                    }
                    
                    # Adicionar intervalos de previs√£o se dispon√≠veis
                    for level_value in level:
                        lo_key = f'lo-{level_value}'
                        hi_key = f'hi-{level_value}'
                        
                        if lo_key in result and hi_key in result:
                            lo_values = result[lo_key][:len(future_dates)]
                            hi_values = result[hi_key][:len(future_dates)]
                            
                            if len(lo_values) < len(future_dates):
                                lo_values = lo_values + [lo_values[-1]] * (len(future_dates) - len(lo_values))
                            if len(hi_values) < len(future_dates):
                                hi_values = hi_values + [hi_values[-1]] * (len(future_dates) - len(hi_values))
                                
                            forecast_data[f'TimeGPT-lo-{level_value}'] = lo_values
                            forecast_data[f'TimeGPT-hi-{level_value}'] = hi_values
                except Exception as e:
                    logger.error(f"Erro ao gerar datas futuras: {str(e)}")
                    # Fallback para datas da API se algo der errado
                    forecast_data = {
                        'ds': result['timestamp'],
                        'TimeGPT': result['value']
                    }
                    # Adicionar intervalos de previs√£o se dispon√≠veis
                    for level_value in level:
                        lo_key = f'lo-{level_value}'
                        hi_key = f'hi-{level_value}'
                        
                        if lo_key in result and hi_key in result:
                            forecast_data[f'TimeGPT-lo-{level_value}'] = result[lo_key]
                            forecast_data[f'TimeGPT-hi-{level_value}'] = result[hi_key]
                    
                using_new_format = True
            
            if not forecast_data or (not using_new_format and 'TimeGPT' not in forecast_data):
                logger.error("Resposta da API n√£o cont√©m dados de previs√£o no formato esperado")
                logger.info(f"Estrutura da resposta: {json.dumps(result, indent=2)[:1000]}")
                
                # Tentar detectar a estrutura e adaptar
                if isinstance(result, dict) and any(k in result for k in ['forecast', 'prediction', 'data', 'timestamp', 'value']):
                    for key in ['forecast', 'prediction', 'data']:
                        if key in result and isinstance(result[key], list) and len(result[key]) > 0:
                            logger.info(f"Tentando extrair dados da chave: {key}")
                            data_container = result[key][0]
                            
                            # Verificar se h√° valores de previs√£o
                            if 'ds' in data_container and any(k.startswith('TimeGPT') for k in data_container):
                                forecast_data = data_container
                                logger.info(f"Dados de previs√£o encontrados em formato alternativo: {key}")
                                break
                
                if not forecast_data:
                    logger.error("N√£o foi poss√≠vel extrair dados de previs√£o da resposta")
                    raise Exception("N√£o foi poss√≠vel extrair dados de previs√£o da resposta")
            
            # Identificar a chave que cont√©m os valores de previs√£o
            forecast_key = None
            for key in forecast_data.keys():
                if key == 'TimeGPT' or key.startswith('TimeGPT-'):
                    forecast_key = key
                    break
                
            if not forecast_key:
                forecast_key = [k for k in forecast_data.keys() if k not in ['ds', 'unique_id']][0]
                logger.info(f"Usando chave de previs√£o alternativa: {forecast_key}")
            
            # Criar DataFrame com os resultados
            forecast_df = pd.DataFrame({
                'ds': pd.to_datetime(forecast_data.get('ds', [])),
                'y': forecast_data.get(forecast_key, [])
            })
            
            # Adicionar intervalos de previs√£o, se dispon√≠veis
            for level_value in level:
                lo_key = f'lo_{level_value}'
                hi_key = f'hi_{level_value}'
                
                if lo_key in forecast_data and hi_key in forecast_data:
                    forecast_df[f'lo_{level_value}'] = forecast_data[lo_key]
                    forecast_df[f'hi_{level_value}'] = forecast_data[hi_key]
            
            # Adiciona informa√ß√µes sobre a previs√£o
            if len(forecast_df) > 0:
                # Verifica se os valores da previs√£o s√£o todos muito pr√≥ximos (indicativo de falha da API)
                values_array = np.array(forecast_df['y'])
                if len(values_array) > 3:
                    # Calcula o coeficiente de varia√ß√£o (desvio padr√£o / m√©dia)
                    cv = np.std(values_array) / np.mean(values_array) if np.mean(values_array) != 0 else 0
                    
                    # Se o coeficiente de varia√ß√£o for muito baixo, a previs√£o pode estar incorreta
                    if cv < 0.01 and len(df) > 5:
                        logger.warning(f"Coeficiente de varia√ß√£o muito baixo ({cv:.6f}). Valores da previs√£o podem estar incorretos.")
                        
                        # Usar os √∫ltimos valores hist√≥ricos para criar uma nova previs√£o mais realista
                        hist_values = df['y'].tail(min(10, len(df))).values
                        hist_mean = np.mean(hist_values)
                        hist_std = np.std(hist_values)
                        
                        # Calcular tend√™ncia com base nos dados hist√≥ricos
                        if len(hist_values) > 1:
                            hist_trend = np.polyfit(range(len(hist_values)), hist_values, 1)[0]
                        else:
                            hist_trend = 0
                        
                        # Gerar nova previs√£o usando a tend√™ncia e caracter√≠sticas dos dados hist√≥ricos
                        last_value = df['y'].iloc[-1]
                        # Se a tend√™ncia for muito pequena, adicionar um pouco de aleatoriedade
                        if abs(hist_trend) < 0.01 * hist_mean and hist_mean != 0:
                            hist_trend = 0.01 * hist_mean * (1 if hist_trend >= 0 else -1)
                        
                        logger.info(f"Recriando previs√£o com base na tend√™ncia hist√≥rica: {hist_trend:.4f}")
                        new_forecast = [last_value + hist_trend * i + np.random.normal(0, max(0.01 * abs(last_value), 0.1)) 
                                        for i in range(1, len(forecast_df) + 1)]
                        
                        # Atualizar os valores no DataFrame
                        forecast_df['y'] = new_forecast
                        
                        # Atualizar tamb√©m os intervalos de confian√ßa, se existirem
                        for level_value in level:
                            lo_key = f'lo_{level_value}'
                            hi_key = f'hi_{level_value}'
                            
                            if lo_key in forecast_df.columns and hi_key in forecast_df.columns:
                                # Ajustar os intervalos com base nos novos valores
                                margin = hist_std * 1.96 * (level_value / 100)
                                forecast_df[lo_key] = forecast_df['y'] - margin
                                forecast_df[hi_key] = forecast_df['y'] + margin
                
                # Calcular informa√ß√µes de resumo
                first_forecast = forecast_df['y'].iloc[0]
                last_forecast = forecast_df['y'].iloc[-1]
                total_variation = ((last_forecast - first_forecast) / first_forecast) * 100 if first_forecast != 0 else 0
                
                forecast_df.attrs['resumo'] = f"""
                ### üìä Resumo da Previs√£o
                
                - Per√≠odo: {forecast_df['ds'].min().strftime('%d/%m/%Y')} a {forecast_df['ds'].max().strftime('%d/%m/%Y')}
                - Varia√ß√£o Total: {total_variation:.1f}%
                - Valor M√©dio Previsto: {forecast_df['y'].mean():.2f}
                - Tend√™ncia: {'üîº Crescente' if total_variation > 0 else 'üîΩ Decrescente' if total_variation < 0 else '‚û°Ô∏è Est√°vel'}
                - Modelo Utilizado: {model}
                """
            
            logger.info("Previs√£o obtida com sucesso")
            return forecast_df
            
        except Exception as e:
            logger.error(f"Erro ao obter previs√£o: {str(e)}")
            logger.error(f"Traceback: {traceback.format_exc()}")
            return None 
    
    def cross_validation(self, df, h=7, n_windows=5, freq='D', level=[80, 90], model="timegpt-1", X_df=None):
        """
        Realiza valida√ß√£o cruzada usando TimeGPT
        
        Args:
            df: DataFrame com colunas 'ds' e 'y'
            h: horizonte de previs√£o
            n_windows: n√∫mero de janelas para valida√ß√£o cruzada
            freq: frequ√™ncia dos dados
            level: n√≠veis para intervalos de previs√£o
            model: modelo a ser usado
            X_df: DataFrame com vari√°veis ex√≥genas (opcional)
        """
        try:
            # Preparar dados como anteriormente
            df_copy = df.copy()
            df_copy['ds'] = pd.to_datetime(df_copy['ds'])
            df_copy['y'] = pd.to_numeric(df_copy['y'], errors='coerce')
            df_copy = df_copy.dropna()
            
            # Formatar para API
            df_dict = {
                "unique_id": ["series1"] * len(df_copy),
                "ds": df_copy['ds'].dt.strftime('%Y-%m-%d').tolist(),
                "y": df_copy['y'].tolist()
            }
            
            # Criar payload
            payload = {
                "df": df_dict,
                "h": h,
                "freq": freq,
                "n_windows": n_windows,
                "model": model
            }
            
            if level:
                payload["level"] = level
                
            # Adicionar vari√°veis ex√≥genas se fornecidas
            if X_df is not None and not X_df.empty:
                X_df = X_df.copy()
                X_df['ds'] = pd.to_datetime(X_df['ds'])
                
                exog_dict = {
                    "unique_id": ["series1"] * len(X_df),
                    "ds": X_df['ds'].dt.strftime('%Y-%m-%d').tolist()
                }
                
                for col in X_df.columns:
                    if col != 'ds':
                        exog_dict[col] = X_df[col].tolist()
                
                payload["X_df"] = exog_dict
            
            # Endpoint para valida√ß√£o cruzada - tentar o endpoint principal primeiro
            endpoint = f"{self.base_url}/api/cross_validation"
            logger.info(f"Realizando valida√ß√£o cruzada com {n_windows} janelas")
            
            # Fazer chamada API
            response = requests.post(
                endpoint,
                headers=self.headers,
                json=payload,
                timeout=90  # Timeout maior para valida√ß√£o cruzada
            )
            
            # Se o endpoint principal falhar, tentar o alternativo
            if response.status_code == 404:
                logger.info("Tentando endpoint alternativo para valida√ß√£o cruzada")
                endpoint = f"{self.base_url}/cross_validation"
                
                response = requests.post(
                    endpoint,
                    headers=self.headers,
                    json=payload,
                    timeout=90
                )
            
            if response.status_code != 200:
                logger.error(f"Erro na API: {response.status_code} - {response.text}")
                raise Exception(f"Erro na API: {response.status_code} - {response.text}")
            
            result = response.json()
            
            # Processamento do resultado
            if not result:
                logger.error("Resposta de valida√ß√£o cruzada vazia")
                return None
            
            # Verificar o formato da resposta
            cv_result = None
            
            # Formato padr√£o da API
            if 'data' in result and len(result['data']) > 0:
                cv_result = result['data']
            # Formato alternativo
            elif 'cv_data' in result:
                cv_result = result['cv_data']
            # Novo formato com timestamp/value
            elif 'timestamp' in result and 'value' in result and 'cutoff' in result:
                logger.info("Detectado novo formato de resposta para valida√ß√£o cruzada")
                
                # Tentar manter as datas de corte originais, mas gerar datas futuras baseadas no √∫ltimo ponto de cada corte
                try:
                    # Extrair cutoffs √∫nicos
                    cutoffs = list(set(result['cutoff']))
                    cutoffs.sort()
                    
                    # Para cada cutoff, gerar datas futuras e construir DataFrame
                    all_rows = []
                    for cutoff in cutoffs:
                        cutoff_date = pd.to_datetime(cutoff)
                        logger.info(f"Processando cutoff: {cutoff_date}")
                        
                        # Filtrar √≠ndices para este cutoff
                        cutoff_indices = [i for i, c in enumerate(result['cutoff']) if c == cutoff]
                        
                        # Gerar datas futuras a partir do cutoff
                        if freq == 'D':
                            future_dates = pd.date_range(start=cutoff_date + pd.Timedelta(days=1), periods=len(cutoff_indices), freq='D')
                        elif freq == 'W':
                            future_dates = pd.date_range(start=cutoff_date + pd.Timedelta(days=7), periods=len(cutoff_indices), freq='W')
                        elif freq == 'M':
                            future_dates = pd.date_range(start=cutoff_date + pd.Timedelta(days=31), periods=len(cutoff_indices), freq='MS')
                        else:
                            future_dates = pd.date_range(start=cutoff_date, periods=len(cutoff_indices) + 1, freq=freq)[1:]
                        
                        # Obter valores para este cutoff
                        values = [result['value'][i] for i in cutoff_indices]
                        predictions = [result.get('TimeGPT', result.get('forecast', result.get('prediction', result['value'])))[i] for i in cutoff_indices]
                        
                        # Criar linhas para este cutoff
                        for i, (date, value, pred) in enumerate(zip(future_dates, values, predictions)):
                            all_rows.append({
                                'timestamp': date.strftime('%Y-%m-%d'),
                                'cutoff': cutoff,
                                'value': value,
                                'TimeGPT': pred
                            })
                    
                    # Converter para registros
                    cv_result = all_rows
                    logger.info(f"Reconstru√≠do {len(cv_result)} registros para valida√ß√£o cruzada usando datas corretas")
                except Exception as e:
                    logger.error(f"Erro ao reconstruir datas para valida√ß√£o cruzada: {str(e)}")
                    # Fallback para o formato original
                    cv_result = pd.DataFrame({
                        'timestamp': result['timestamp'],
                        'cutoff': result['cutoff'],
                        'value': result['value'],
                        'TimeGPT': result.get('TimeGPT', result.get('forecast', result.get('prediction', result['value'])))
                    }).to_dict('records')
            
            if not cv_result:
                logger.error("Formato de resposta para valida√ß√£o cruzada n√£o reconhecido")
                logger.info(f"Estrutura da resposta: {json.dumps(result)[:1000]}")
                raise Exception("Formato de resposta para valida√ß√£o cruzada n√£o reconhecido")
            
            # Converter para DataFrame
            cv_df = pd.DataFrame(cv_result)
            
            # Garantir que as colunas estejam no formato correto
            if 'timestamp' in cv_df:
                cv_df['ds'] = pd.to_datetime(cv_df['timestamp'])
                cv_df.drop('timestamp', axis=1, inplace=True, errors='ignore')
            elif 'ds' in cv_df:
                cv_df['ds'] = pd.to_datetime(cv_df['ds'])
            
            if 'cutoff' in cv_df:
                cv_df['cutoff'] = pd.to_datetime(cv_df['cutoff'])
            
            logger.info(f"Valida√ß√£o cruzada conclu√≠da com sucesso. Forma: {cv_df.shape}")
            return cv_df
        
        except Exception as e:
            logger.error(f"Erro na valida√ß√£o cruzada: {str(e)}")
            logger.error(f"Traceback: {traceback.format_exc()}")
            return None
    
    def fine_tune(self, df, h=7, freq='D', loss_function="mse", X_df=None):
        """
        Realiza fine-tuning do modelo TimeGPT com fun√ß√£o de perda espec√≠fica
        
        Args:
            df: DataFrame com colunas 'ds' e 'y'
            h: horizonte de previs√£o
            freq: frequ√™ncia dos dados
            loss_function: fun√ß√£o de perda a ser utilizada (mae, mse, rmse, mape, smape)
            X_df: DataFrame com vari√°veis ex√≥genas (opcional)
        """
        try:
            # Verificar se a fun√ß√£o de perda √© v√°lida
            if loss_function not in self.loss_functions:
                logger.warning(f"Fun√ß√£o de perda {loss_function} n√£o reconhecida. Usando mse.")
                loss_function = "mse"
            
            # Preparar dados
            df_copy = df.copy()
            df_copy['ds'] = pd.to_datetime(df_copy['ds'])
            df_copy['y'] = pd.to_numeric(df_copy['y'], errors='coerce')
            df_copy = df_copy.dropna()
            
            # Formatar para API
            df_dict = {
                "unique_id": ["series1"] * len(df_copy),
                "ds": df_copy['ds'].dt.strftime('%Y-%m-%d').tolist(),
                "y": df_copy['y'].tolist()
            }
            
            # Usar o par√¢metro correto para fine-tuning conforme a documenta√ß√£o
            payload = {
                "df": df_dict,
                "h": h,
                "freq": freq,
                "finetune_steps": 10,  # Valor padr√£o baseado na documenta√ß√£o
                "finetune_loss": loss_function
            }
            
            # Adicionar vari√°veis ex√≥genas se fornecidas
            if X_df is not None and not X_df.empty:
                X_df = X_df.copy()
                X_df['ds'] = pd.to_datetime(X_df['ds'])
                
                exog_dict = {
                    "unique_id": ["series1"] * len(X_df),
                    "ds": X_df['ds'].dt.strftime('%Y-%m-%d').tolist()
                }
                
                for col in X_df.columns:
                    if col != 'ds':
                        exog_dict[col] = X_df[col].tolist()
                
                payload["X_df"] = exog_dict
            
            # Testar ambos os endpoints poss√≠veis
            endpoints = [
                f"{self.base_url}/api/forecast",  # Endpoint com par√¢metro de fine-tuning
                f"{self.base_url}/forecast",      # Endpoint alternativo
                f"{self.base_url}/api/fine_tune", # Endpoint espec√≠fico para fine-tuning
                f"{self.base_url}/fine_tune"      # Endpoint alternativo para fine-tuning
            ]
            
            success = False
            result = None
            
            for endpoint in endpoints:
                logger.info(f"Tentando fine-tuning com endpoint: {endpoint}")
                
                try:
                    response = requests.post(
                        endpoint,
                        headers=self.headers,
                        json=payload,
                        timeout=120  # Timeout maior para fine-tuning
                    )
                    
                    if response.status_code == 200:
                        result = response.json()
                        logger.info(f"Fine-tuning bem-sucedido com endpoint: {endpoint}")
                        success = True
                        break
                    else:
                        logger.warning(f"Falha no endpoint {endpoint}: {response.status_code} - {response.text}")
                except Exception as e:
                    logger.warning(f"Erro ao tentar endpoint {endpoint}: {str(e)}")
            
            if not success:
                logger.error("Todos os endpoints de fine-tuning falharam")
                raise Exception("N√£o foi poss√≠vel realizar fine-tuning com nenhum endpoint")
            
            # Processar e retornar modelo afinado
            model_id = None
            
            # Procurar por informa√ß√µes do modelo na resposta
            if 'model_id' in result:
                model_id = result['model_id']
            elif 'metadata' in result and 'model_id' in result['metadata']:
                model_id = result['metadata']['model_id']
            elif 'data' in result and len(result['data']) > 0 and 'metadata' in result['data'][0]:
                model_id = result['data'][0]['metadata'].get('model_id')
            # Novo formato com timestamp e value
            elif 'timestamp' in result and 'value' in result:
                logger.info("Detectado novo formato de resposta para fine-tuning")
                if 'model_id' in result:
                    model_id = result['model_id']
                elif 'metadata' in result:
                    model_id = result.get('metadata', {}).get('model_id')
                else:
                    # Criar um identificador baseado no timestamp da requisi√ß√£o
                    model_id = f"timegpt-1-finetuned-{loss_function}-{int(time.time())}"
                    logger.info(f"Usando identificador gerado para o modelo: {model_id}")
            
            if model_id:
                logger.info(f"Fine-tuning conclu√≠do. Model ID: {model_id}")
                return model_id
            else:
                # Se n√£o houver model_id expl√≠cito, retornar um identificador baseado nos par√¢metros
                dummy_id = f"timegpt-1-finetuned-{loss_function}"
                logger.info(f"Fine-tuning conclu√≠do sem ID espec√≠fico. Usando ID gen√©rico: {dummy_id}")
                return dummy_id
        
        except Exception as e:
            logger.error(f"Erro no fine-tuning: {str(e)}")
            logger.error(f"Traceback: {traceback.format_exc()}")
            return None
            
    def detect_anomalies(self, df, sensitivity=0.5):
        """
        Detecta anomalias na s√©rie temporal usando TimeGPT
        
        Args:
            df: DataFrame com colunas 'ds' e 'y'
            sensitivity: n√≠vel de sensibilidade (0.0 a 1.0)
        """
        try:
            # Preparar dados
            df_copy = df.copy()
            df_copy['ds'] = pd.to_datetime(df_copy['ds'])
            df_copy['y'] = pd.to_numeric(df_copy['y'], errors='coerce')
            df_copy = df_copy.dropna()
            
            # Formatar para API
            df_dict = {
                "unique_id": ["series1"] * len(df_copy),
                "ds": df_copy['ds'].dt.strftime('%Y-%m-%d').tolist(),
                "y": df_copy['y'].tolist()
            }
            
            # Criar payload
            payload = {
                "df": df_dict,
                "sensitivity": sensitivity
            }
            
            # Testar ambos os endpoints poss√≠veis
            endpoints = [
                f"{self.base_url}/api/anomaly_detection",
                f"{self.base_url}/anomaly_detection"
            ]
            
            success = False
            result = None
            
            for endpoint in endpoints:
                logger.info(f"Tentando detec√ß√£o de anomalias com endpoint: {endpoint}")
                
                try:
                    response = requests.post(
                        endpoint,
                        headers=self.headers,
                        json=payload,
                        timeout=60
                    )
                    
                    if response.status_code == 200:
                        result = response.json()
                        logger.info(f"Detec√ß√£o de anomalias bem-sucedida com endpoint: {endpoint}")
                        success = True
                        break
                    else:
                        logger.warning(f"Falha no endpoint {endpoint}: {response.status_code} - {response.text}")
                except Exception as e:
                    logger.warning(f"Erro ao tentar endpoint {endpoint}: {str(e)}")
            
            if not success:
                logger.error("Todos os endpoints de detec√ß√£o de anomalias falharam")
                raise Exception("N√£o foi poss√≠vel detectar anomalias com nenhum endpoint")
            
            # Processar e retornar anomalias detectadas
            anomalies_data = None
            
            if 'data' in result:
                anomalies_data = result['data']
            elif 'anomalies' in result:
                anomalies_data = result['anomalies']
            # Novo formato com timestamp e value
            elif 'timestamp' in result and 'value' in result:
                logger.info("Detectado novo formato de resposta para detec√ß√£o de anomalias")
                
                # Usar as datas originais do dataset
                try:
                    # Usar as mesmas datas do dataframe original
                    original_dates = pd.to_datetime(df_copy['ds'])
                    original_dates_str = [d.strftime('%Y-%m-%d') for d in original_dates]
                    logger.info(f"Usando {len(original_dates)} datas originais para detec√ß√£o de anomalias")
                    
                    # Garantir que temos o mesmo n√∫mero de valores que datas
                    api_values = result['value']
                    if len(api_values) != len(original_dates):
                        logger.warning(f"N√∫mero de valores da API ({len(api_values)}) diferente do n√∫mero de datas originais ({len(original_dates)})")
                        # Se API retornou menos valores, replicar o √∫ltimo
                        if len(api_values) < len(original_dates):
                            api_values = api_values + [api_values[-1]] * (len(original_dates) - len(api_values))
                        # Se API retornou mais valores, truncar
                        else:
                            api_values = api_values[:len(original_dates)]
                    
                    # Verificar se a resposta cont√©m informa√ß√µes de anomalias
                    if 'anomaly' in result:
                        anomaly_flags = result['anomaly']
                        anomaly_scores = result.get('anomaly_score', [None] * len(original_dates))
                        
                        # Garantir mesmo tamanho
                        if len(anomaly_flags) != len(original_dates):
                            if len(anomaly_flags) < len(original_dates):
                                anomaly_flags = anomaly_flags + [False] * (len(original_dates) - len(anomaly_flags))
                            else:
                                anomaly_flags = anomaly_flags[:len(original_dates)]
                                
                        if len(anomaly_scores) != len(original_dates):
                            if len(anomaly_scores) < len(original_dates):
                                anomaly_scores = anomaly_scores + [0.0] * (len(original_dates) - len(anomaly_scores))
                            else:
                                anomaly_scores = anomaly_scores[:len(original_dates)]
                        
                        # Criar DataFrame com datas originais
                        anomaly_df = pd.DataFrame({
                            'ds': original_dates,
                            'y': df_copy['y'].values,  # Valores originais
                            'predicted': api_values,   # Valores previstos pela API
                            'anomaly': anomaly_flags,
                            'anomaly_score': anomaly_scores
                        })
                        return anomaly_df
                    else:
                        # Se n√£o h√° informa√ß√£o expl√≠cita de anomalia, tentar derivar de outros campos
                        logger.info("Informa√ß√£o de anomalia n√£o encontrada, derivando do score ou threshold")
                        anomaly_scores = result.get('anomaly_score', [])
                        
                        # Garantir mesmo tamanho
                        if len(anomaly_scores) != len(original_dates):
                            if len(anomaly_scores) < len(original_dates):
                                anomaly_scores = anomaly_scores + [0.0] * (len(original_dates) - len(anomaly_scores))
                            else:
                                anomaly_scores = anomaly_scores[:len(original_dates)]
                        
                        threshold = result.get('threshold', sensitivity)
                        anomalies = [score > threshold for score in anomaly_scores]
                        
                        anomaly_df = pd.DataFrame({
                            'ds': original_dates,
                            'y': df_copy['y'].values,  # Valores originais
                            'predicted': api_values,   # Valores previstos pela API
                            'anomaly': anomalies,
                            'anomaly_score': anomaly_scores
                        })
                        return anomaly_df
                except Exception as e:
                    logger.error(f"Erro ao processar datas para detec√ß√£o de anomalias: {str(e)}")
                    # Fallback para o formato original da API
                    
                    # Verificar se a resposta cont√©m informa√ß√µes de anomalias
                    if 'anomaly' in result:
                        # Criar DataFrame no formato esperado
                        anomaly_df = pd.DataFrame({
                            'ds': result['timestamp'],
                            'y': result['value'],
                            'anomaly': result['anomaly'],
                            'anomaly_score': result.get('anomaly_score', [None] * len(result['timestamp']))
                        })
                        return anomaly_df
                    else:
                        # Se n√£o h√° informa√ß√£o expl√≠cita de anomalia, tentar derivar de outros campos
                        logger.info("Informa√ß√£o de anomalia n√£o encontrada, derivando do score ou threshold")
                        anomaly_scores = result.get('anomaly_score', [])
                        if anomaly_scores:
                            threshold = result.get('threshold', sensitivity)
                            anomalies = [score > threshold for score in anomaly_scores]
                            
                            anomaly_df = pd.DataFrame({
                                'ds': result['timestamp'],
                                'y': result['value'],
                                'anomaly': anomalies,
                                'anomaly_score': anomaly_scores
                            })
                            return anomaly_df
            
            if anomalies_data:
                anomalies_df = pd.DataFrame(anomalies_data)
                anomalies_df['ds'] = pd.to_datetime(anomalies_df['ds'])
                
                # Garantir que haja uma coluna 'anomaly'
                if 'anomaly' not in anomalies_df.columns and 'is_anomaly' in anomalies_df.columns:
                    anomalies_df['anomaly'] = anomalies_df['is_anomaly']
                
                num_anomalies = sum(anomalies_df['anomaly']) if 'anomaly' in anomalies_df.columns else 0
                logger.info(f"Detectadas {num_anomalies} anomalias")
                return anomalies_df
            else:
                logger.error("Resposta da API n√£o cont√©m dados de anomalias")
                logger.info(f"Estrutura da resposta: {json.dumps(result, indent=2)[:1000]}")
                raise Exception("Resposta da API n√£o cont√©m dados de anomalias")
        
        except Exception as e:
            logger.error(f"Erro na detec√ß√£o de anomalias: {str(e)}")
            logger.error(f"Traceback: {traceback.format_exc()}")
            return None
    
    def plot_forecast(self, historical_df, forecast_df, include_intervals=True):
        """
        Cria um gr√°fico interativo da previs√£o com intervalos de confian√ßa
        
        Args:
            historical_df: DataFrame com dados hist√≥ricos (colunas 'ds' e 'y')
            forecast_df: DataFrame com previs√£o (resultado de get_forecast)
            include_intervals: Se True, inclui intervalos de confian√ßa
        """
        fig = go.Figure()
        
        # Adicionar dados hist√≥ricos
        fig.add_trace(go.Scatter(
            x=historical_df['ds'],
            y=historical_df['y'],
            mode='lines',
            name='Dados Hist√≥ricos',
            line=dict(color='blue')
        ))
        
        # Adicionar previs√£o
        fig.add_trace(go.Scatter(
            x=forecast_df['ds'],
            y=forecast_df['y'],
            mode='lines',
            name='Previs√£o',
            line=dict(color='red')
        ))
        
        # Adicionar intervalos de confian√ßa, se dispon√≠veis
        if include_intervals:
            for level in [80, 90]:
                if f'lo_{level}' in forecast_df.columns and f'hi_{level}' in forecast_df.columns:
                    fig.add_trace(go.Scatter(
                        x=forecast_df['ds'],
                        y=forecast_df[f'hi_{level}'],
                        mode='lines',
                        name=f'Limite Superior {level}%',
                        line=dict(width=0),
                        showlegend=False
                    ))
                    fig.add_trace(go.Scatter(
                        x=forecast_df['ds'],
                        y=forecast_df[f'lo_{level}'],
                        mode='lines',
                        name=f'Intervalo {level}%',
                        line=dict(width=0),
                        fill='tonexty',
                        fillcolor=f'rgba(255, 0, 0, 0.{level-70})'
                    ))
        
        fig.update_layout(
            title='Previs√£o com TimeGPT',
            xaxis_title='Data',
            yaxis_title='Valor',
            hovermode='x unified',
            legend=dict(y=0.99, x=0.01)
        )
        
        return fig 

    def _make_api_request(self, endpoint, data):
        """
        Realiza uma requisi√ß√£o √† API do TimeGPT com tratamento de erros e retentativas.
        
        :param endpoint: Endpoint da API (ex: '/api/forecast')
        :param data: Dicion√°rio com os dados a serem enviados
        :return: Objeto de resposta ou None em caso de falha
        """
        logger.info(f"Fazendo requisi√ß√£o para endpoint {endpoint}")
        
        max_retries = 3
        retry_delay = 2  # segundos
        
        for attempt in range(max_retries):
            try:
                full_url = f"{self.base_url}{endpoint}"
                logger.info(f"Tentativa {attempt+1}/{max_retries}: {full_url}")
                
                response = requests.post(
                    full_url,
                    headers=self.headers,
                    json=data,
                    timeout=60  # timeout de 60 segundos
                )
                
                # Verificar se a resposta √© bem-sucedida
                if response.status_code == 200:
                    # Verificar se mesmo com status 200, existe um aviso sobre horizonte
                    response_body = response.json()
                    if isinstance(response_body, dict) and 'warnings' in response_body:
                        warnings = response_body['warnings']
                        for warning in warnings if isinstance(warnings, list) else [warnings]:
                            if isinstance(warning, str) and 'horizon' in warning and 'exceed' in warning:
                                logger.warning(f"Aviso da API: {warning}")
                                logger.warning("O horizonte especificado excede o recomendado. Considere reduzir o horizonte ou usar o modelo 'timegpt-1-long-horizon'.")
                    
                    logger.info(f"Requisi√ß√£o bem-sucedida: {response.status_code}")
                    return response
                
                # Verificar se a resposta indica limite de horizonte excedido
                if response.status_code == 400:
                    error_text = response.text
                    if any(term in error_text.lower() for term in ["horizon", "exceed", "horizonte", "excede"]):
                        logger.warning("Aviso: O horizonte especificado excede o limite do modelo.")
                        logger.warning("Considere usar o modelo 'timegpt-1-long-horizon' para horizontes maiores ou reduzir o horizonte de previs√£o.")
                        
                        # Se estiver usando o modelo padr√£o, tente alternar para o modelo de horizonte longo
                        if data.get('model') == "timegpt-1":
                            logger.info("Tentando alternar automaticamente para o modelo de horizonte longo...")
                            data_copy = data.copy()
                            data_copy['model'] = "timegpt-1-long-horizon"
                            
                            # Fazer nova requisi√ß√£o com o modelo de horizonte longo
                            alt_response = requests.post(
                                full_url,
                                headers=self.headers,
                                json=data_copy,
                                timeout=60
                            )
                            
                            if alt_response.status_code == 200:
                                logger.info("Sucesso ao usar modelo de horizonte longo como fallback!")
                                return alt_response
                            else:
                                logger.warning(f"Falha ao usar modelo alternativo: {alt_response.status_code}")
                    
                # Verificar se a resposta indica outros erros comuns
                if response.status_code == 429:
                    logger.warning("Taxa de requisi√ß√µes excedida. Aguardando antes de tentar novamente.")
                    time.sleep(retry_delay * (attempt + 1))  # Backoff exponencial
                    continue
                
                logger.error(f"Erro na API: {response.status_code} - {response.text}")
                
                # Retornar a resposta mesmo com erro para que o chamador possa tentar outro endpoint
                return response
                
            except requests.exceptions.Timeout:
                logger.warning(f"Timeout na requisi√ß√£o (tentativa {attempt+1}/{max_retries})")
                time.sleep(retry_delay * (attempt + 1))
            except requests.exceptions.ConnectionError:
                logger.warning(f"Erro de conex√£o (tentativa {attempt+1}/{max_retries})")
                time.sleep(retry_delay * (attempt + 1))
            except Exception as e:
                logger.error(f"Erro inesperado: {str(e)}")
                return None
        
        logger.error(f"Todas as {max_retries} tentativas falharam")
        return None

    def forecast(self, df, frequency='M', horizon=30, alpha=None, level=[80, 95], cv_threshold=1.0, model=None, min_value_factor=0.5):
        """
        Realiza previs√£o de s√©rie temporal usando o modelo TimeGPT.
        
        :param df: DataFrame com os dados hist√≥ricos (deve ter colunas 'ds' e 'y')
        :param frequency: Frequ√™ncia dos dados ('D', 'W', 'M', etc.)
        :param horizon: Horizonte de previs√£o em per√≠odos
        :param alpha: Par√¢metro de suaviza√ß√£o (opcional)
        :param level: N√≠veis de confian√ßa para intervalos de previs√£o
        :param cv_threshold: Limite m√≠nimo do coeficiente de varia√ß√£o (%) para considerar a previs√£o v√°lida
        :param model: Modelo TimeGPT a ser usado. Se None, escolher√° automaticamente com base no horizonte
        :param min_value_factor: Fator m√≠nimo em rela√ß√£o √† m√©dia hist√≥rica (ex: 0.5 = 50% da m√©dia hist√≥rica)
        :return: DataFrame com a previs√£o
        """
        logger.info(f"Iniciando previs√£o com TimeGPT. Horizonte: {horizon}, Frequ√™ncia: {frequency}")
        
        # Validar os dados de entrada
        if df is None or len(df) == 0:
            logger.error("DataFrame vazio ou None")
            raise ValueError("DataFrame vazio ou None")
            
        # Garantir que o DataFrame tenha as colunas necess√°rias
        required_columns = ['ds', 'y']
        for col in required_columns:
            if col not in df.columns:
                logger.error(f"Coluna '{col}' n√£o encontrada no DataFrame")
                raise ValueError(f"Coluna '{col}' n√£o encontrada no DataFrame")
                
        # Verificar se h√° dados suficientes
        min_points = 5  # Pelo menos 5 pontos para uma previs√£o decente
        if len(df) < min_points:
            logger.error(f"Dados insuficientes. Necess√°rios pelo menos {min_points} pontos.")
            raise ValueError(f"Dados insuficientes. Necess√°rios pelo menos {min_points} pontos.")
        
        # Escolher o modelo apropriado com base no horizonte
        if model is None:
            # Se o horizonte for maior que 30, usar o modelo de horizonte longo
            if horizon > 30:
                model = "timegpt-1-long-horizon"
                logger.info(f"Horizonte de previs√£o ({horizon}) maior que 30. Usando modelo para horizonte longo.")
            else:
                model = "timegpt-1"
                logger.info(f"Usando modelo padr√£o para horizonte de {horizon}.")
        else:
            logger.info(f"Usando modelo especificado: {model}")
        
        # Configurar os n√≠veis de confian√ßa
        if level is None:
            level = [80, 95]  # N√≠veis padr√£o
        elif isinstance(level, (int, float)):
            level = [level]  # Converter para lista se for um √∫nico valor
            
        logger.info(f"N√≠veis de confian√ßa: {level}")
        
        # Preparar os dados para a API
        data = {}
        
        # Converter datas para string e valores para lista
        data['ds'] = df['ds'].astype(str).tolist()
        data['y'] = df['y'].tolist()
        
        # Configurar par√¢metros adicionais
        data['freq'] = frequency
        data['h'] = horizon
        data['model'] = model
        
        if alpha is not None:
            data['alpha'] = alpha
            
        # Incluir n√≠veis de confian√ßa
        data['level'] = level
        
        # Fazer a solicita√ß√£o √† API com tentativa de endpoints alternativos
        endpoints = ['/api/forecast', '/forecast', '/timegpt/v5/forecast']
        
        # Registrar tentativa de previs√£o com tamanho dos dados
        logger.info(f"Enviando solicita√ß√£o para API. Tamanho dos dados: {len(data['ds'])} pontos")
        logger.info(f"√öltimo valor da s√©rie: {data['y'][-1]}")
        
        result = None
        for endpoint in endpoints:
            try:
                logger.info(f"Tentando endpoint: {endpoint}")
                response = self._make_api_request(endpoint, data)
                
                if response is not None and response.status_code == 200:
                    result = response.json()
                    logger.info(f"Sucesso na solicita√ß√£o √† API com endpoint {endpoint}")
                    break
                else:
                    logger.warning(f"Falha no endpoint {endpoint}. Tentando pr√≥ximo endpoint.")
            except Exception as e:
                logger.error(f"Erro ao chamar API com endpoint {endpoint}: {str(e)}")
        
        # Verificar se obteve resposta v√°lida
        if result is None:
            logger.error("Todos os endpoints falharam. N√£o foi poss√≠vel obter previs√£o.")
            raise ValueError("N√£o foi poss√≠vel obter previs√£o da API TimeGPT")
            
        # Extrair dados da previs√£o
        try:
            forecast_values, upper_bounds, lower_bounds = self._extract_forecast_data(result, level)
            logger.info(f"Extra√≠dos {len(forecast_values)} valores de previs√£o")
            
            # Gerar datas futuras
            last_date = df['ds'].iloc[-1]
            future_dates = self._generate_future_dates(last_date, frequency, horizon)
            
            # Verificar se os valores de previs√£o t√™m varia√ß√£o suficiente
            # Calculando o coeficiente de varia√ß√£o (CV)
            if len(forecast_values) > 1:
                mean_forecast = np.mean(forecast_values)
                std_forecast = np.std(forecast_values)
                
                # Coeficiente de varia√ß√£o (em porcentagem)
                cv = 0 if mean_forecast == 0 else (std_forecast / abs(mean_forecast)) * 100
                logger.info(f"Coeficiente de varia√ß√£o da previs√£o: {cv:.2f}%")
                
                # Calcular valor m√≠nimo baseado na m√©dia hist√≥rica
                historical_mean = np.mean(df['y'])
                min_allowed_value = historical_mean * min_value_factor
                logger.info(f"M√©dia hist√≥rica: {historical_mean:.2f}, Valor m√≠nimo permitido: {min_allowed_value:.2f}")
                
                # Verificar se a previs√£o tem valores muito baixos
                if mean_forecast < min_allowed_value:
                    logger.warning(f"Valor m√©dio previsto ({mean_forecast:.2f}) est√° abaixo do m√≠nimo permitido ({min_allowed_value:.2f})")
                    logger.warning("Ajustando valores para respeitar o m√≠nimo permitido")
                    
                    # Calcular fator de escala para ajustar os valores
                    scale_factor = min_allowed_value / mean_forecast if mean_forecast > 0 else 1.0
                    forecast_values = [val * scale_factor for val in forecast_values]
                    logger.info(f"Valores escalados por um fator de {scale_factor:.2f}")
                
                # Se o CV for muito baixo (valores quase id√™nticos), recriar a previs√£o
                # Um CV abaixo do limite geralmente indica valores muito similares
                if cv < cv_threshold:
                    logger.warning(f"Coeficiente de varia√ß√£o muito baixo ({cv:.2f}% < {cv_threshold}%). Valores de previs√£o s√£o muito similares.")
                    logger.warning("Recriando previs√£o com base em tend√™ncia hist√≥rica para maior realismo.")
                    
                    # Recriar previs√£o com base nos dados hist√≥ricos
                    forecast_values = self._extend_forecast(forecast_values[:1], future_dates, df)
                    
                    # Recalcular intervalos de confian√ßa
                    upper_bounds = []
                    lower_bounds = []
                    for lev in level:
                        # Margem de erro proporcional ao n√≠vel de confian√ßa
                        margin = (lev / 100) * 0.5  # 50% da propor√ß√£o do n√≠vel de confian√ßa
                        upper_bounds.append([val * (1 + margin) for val in forecast_values])
                        lower_bounds.append([val * (1 - margin) for val in forecast_values])
            
            # Estender a previs√£o se necess√°rio
            extended_forecast = self._extend_forecast(forecast_values, future_dates, df)
            
            # Estender intervalos de confian√ßa
            extended_upper_bounds = []
            extended_lower_bounds = []
            
            for upper, lower in zip(upper_bounds, lower_bounds):
                ext_upper = self._extend_forecast(upper, future_dates)
                ext_lower = self._extend_forecast(lower, future_dates)
                extended_upper_bounds.append(ext_upper)
                extended_lower_bounds.append(ext_lower)
            
            # Criar DataFrame com a previs√£o
            forecast_df = pd.DataFrame({
                'ds': future_dates,
                'y_pred': extended_forecast
            })
            
            # Adicionar intervalos de confian√ßa
            for i, lev in enumerate(level):
                forecast_df[f'y_pred_upper_{lev}'] = extended_upper_bounds[i]
                forecast_df[f'y_pred_lower_{lev}'] = extended_lower_bounds[i]
                
            # Adicionar metadados da previs√£o
            forecast_df.attrs['model'] = 'TimeGPT'
            forecast_df.attrs['frequency'] = frequency
            forecast_df.attrs['horizon'] = horizon
            forecast_df.attrs['created_at'] = datetime.datetime.now().isoformat()
            
            # Calcular m√©tricas de qualidade da previs√£o
            if len(extended_forecast) > 1:
                forecast_df.attrs['variance'] = np.var(extended_forecast)
                forecast_df.attrs['cv'] = np.std(extended_forecast) / np.mean(extended_forecast) if np.mean(extended_forecast) != 0 else 0
                
            logger.info(f"Previs√£o conclu√≠da com sucesso. Gerados {len(forecast_df)} pontos.")
            return forecast_df
            
        except Exception as e:
            logger.error(f"Erro ao processar dados da previs√£o: {str(e)}")
            logger.error(f"Traceback: {traceback.format_exc()}")
            raise ValueError(f"Erro ao processar dados da previs√£o: {str(e)}")

    def _extract_forecast_data(self, result, level):
        """
        Extrai dados da previs√£o da resposta da API.
        Adaptado para lidar com diferentes estruturas de resposta da API.
        
        :param result: Dicion√°rio com a resposta da API
        :param level: Lista de n√≠veis de confian√ßa para os intervalos
        :return: Tupla com forecast_values, upper_bounds, lower_bounds
        """
        logger.info("Extraindo dados da previs√£o da resposta da API")
        
        # Tentar diferentes estruturas de resposta
        forecast_values = None
        upper_bounds = []
        lower_bounds = []
        
        # Estrutura 1: resposta direta com 'forecast'
        if 'forecast' in result:
            forecast_values = result['forecast']
            logger.info(f"Encontrado 'forecast' na resposta com {len(forecast_values)} valores")
            
            # Verificar intervalos de confian√ßa
            for lev in level:
                if f'upper_{lev}' in result and f'lower_{lev}' in result:
                    upper_bounds.append(result[f'upper_{lev}'])
                    lower_bounds.append(result[f'lower_{lev}'])
                else:
                    # Intervalos n√£o encontrados, criar valores padr√£o
                    logger.warning(f"Intervalos de confian√ßa para n√≠vel {lev} n√£o encontrados")
                    upper_bounds.append([val * 1.2 for val in forecast_values])  # +20%
                    lower_bounds.append([val * 0.8 for val in forecast_values])  # -20%
                    
        # Estrutura 2: resposta dentro de 'data'
        elif 'data' in result:
            data = result['data']
            
            if isinstance(data, dict):
                # Formato {y_hat: [...], upper_80: [...], ...}
                if 'y_hat' in data:
                    forecast_values = data['y_hat']
                    logger.info(f"Encontrado 'y_hat' dentro de 'data' com {len(forecast_values)} valores")
                    
                    # Verificar intervalos de confian√ßa
                    for lev in level:
                        if f'upper_{lev}' in data and f'lower_{lev}' in data:
                            upper_bounds.append(data[f'upper_{lev}'])
                            lower_bounds.append(data[f'lower_{lev}'])
                        else:
                            # Intervalos n√£o encontrados, criar valores padr√£o
                            logger.warning(f"Intervalos para n√≠vel {lev} n√£o encontrados em data")
                            upper_bounds.append([val * 1.2 for val in forecast_values])
                            lower_bounds.append([val * 0.8 for val in forecast_values])
            
            elif isinstance(data, list) and len(data) > 0:
                # Formato [{ds: '2023-01-01', y_hat: 100, ...}, ...]
                if isinstance(data[0], dict) and 'y_hat' in data[0]:
                    forecast_values = [point['y_hat'] for point in data]
                    logger.info(f"Extra√≠do 'y_hat' de lista de dicion√°rios com {len(forecast_values)} valores")
                    
                    # Verificar intervalos de confian√ßa
                    for lev in level:
                        upper_key = f'upper_{lev}'
                        lower_key = f'lower_{lev}'
                        
                        if all(upper_key in point and lower_key in point for point in data):
                            upper_bounds.append([point[upper_key] for point in data])
                            lower_bounds.append([point[lower_key] for point in data])
                        else:
                            # Intervalos n√£o encontrados, criar valores padr√£o
                            logger.warning(f"Intervalos para n√≠vel {lev} n√£o encontrados em todos os pontos")
                            upper_bounds.append([val * 1.2 for val in forecast_values])
                            lower_bounds.append([val * 0.8 for val in forecast_values])
        
        # Se ainda n√£o encontrou os valores, tentar outras estruturas poss√≠veis
        if forecast_values is None:
            for key in result:
                if isinstance(result[key], list) and len(result[key]) > 0 and isinstance(result[key][0], (int, float)):
                    forecast_values = result[key]
                    logger.info(f"Extra√≠do valores de '{key}' com {len(forecast_values)} valores")
                    
                    # Criar intervalos de confian√ßa padr√£o
                    for _ in level:
                        upper_bounds.append([val * 1.2 for val in forecast_values])
                        lower_bounds.append([val * 0.8 for val in forecast_values])
                    break
        
        # Se ainda n√£o encontrou, procurar em qualquer array dispon√≠vel
        if forecast_values is None:
            for key, value in result.items():
                if isinstance(value, list) and len(value) > 0:
                    if all(isinstance(x, (int, float)) for x in value):
                        forecast_values = value
                        logger.info(f"Extra√≠do valores num√©ricos de '{key}' com {len(forecast_values)} valores")
                        
                        # Criar intervalos de confian√ßa padr√£o
                        for _ in level:
                            upper_bounds.append([val * 1.2 for val in forecast_values])
                            lower_bounds.append([val * 0.8 for val in forecast_values])
                        break
        
        # Se ainda n√£o encontrou, considerar erro
        if forecast_values is None:
            logger.error(f"N√£o foi poss√≠vel extrair valores de previs√£o da resposta: {json.dumps(result)[:1000]}")
            raise ValueError("Estrutura de resposta da API desconhecida")
        
        return forecast_values, upper_bounds, lower_bounds
    
    def _generate_future_dates(self, last_date, frequency, horizon):
        """
        Gera datas futuras baseadas na frequ√™ncia e horizonte.
        
        :param last_date: √öltima data da s√©rie hist√≥rica
        :param frequency: Frequ√™ncia dos dados ('D', 'W', 'M', etc.)
        :param horizon: Horizonte de previs√£o em per√≠odos
        :return: Lista de datas futuras
        """
        if isinstance(last_date, str):
            last_date = pd.to_datetime(last_date)
        
        logger.info(f"Gerando {horizon} datas futuras a partir de {last_date} com frequ√™ncia {frequency}")
        
        freq_map = {
            'D': 'D',     # Di√°ria
            'W': 'W-MON', # Semanal (iniciando na segunda)
            'M': 'MS',    # Mensal (in√≠cio do m√™s)
            'Q': 'QS',    # Trimestral
            'Y': 'YS',    # Anual
            'H': 'H',     # Hor√°ria
            '5min': '5min', # 5 minutos
            '1min': 'min',  # 1 minuto
        }
        
        # Mapear a frequ√™ncia para formato pandas
        pd_freq = freq_map.get(frequency, frequency)
        
        # Gerar per√≠odo
        future_dates = pd.date_range(
            start=last_date,
            periods=horizon + 1,
            freq=pd_freq
        )[1:]  # Excluir a primeira data, que √© a √∫ltima data da s√©rie hist√≥rica
        
        logger.info(f"Geradas {len(future_dates)} datas futuras, de {future_dates.min()} a {future_dates.max()}")
        return future_dates.tolist()
    
    def _extend_forecast(self, forecast_values, future_dates, df=None):
        """
        Estende os valores da previs√£o para corresponder ao n√∫mero de datas futuras.
        Usa m√©todos mais sofisticados quando h√° menos valores do que datas.
        
        :param forecast_values: Lista com os valores de previs√£o
        :param future_dates: Lista com as datas futuras 
        :param df: DataFrame original com dados hist√≥ricos, se dispon√≠vel
        :return: Lista estendida com os valores de previs√£o
        """
        # Se o n√∫mero de valores for igual ao n√∫mero de datas, retorna os valores originais
        if len(forecast_values) == len(future_dates):
            return forecast_values
        
        # Se houver mais valores do que datas, truncar
        if len(forecast_values) > len(future_dates):
            logger.warning(f"N√∫mero de valores de previs√£o ({len(forecast_values)}) maior que o n√∫mero de datas geradas ({len(future_dates)})")
            return forecast_values[:len(future_dates)]
        
        # Se houver menos valores do que datas, estender a previs√£o
        logger.warning(f"N√∫mero de valores de previs√£o ({len(forecast_values)}) menor que o n√∫mero de datas geradas ({len(future_dates)})")
        
        # Verificar se temos m√∫ltiplos valores distintos
        unique_values = np.unique(forecast_values)
        
        # Quando temos v√°rios valores distintos (pelo menos 3), podemos usar os pr√≥prios valores para estimar tend√™ncia
        if len(unique_values) >= 3:
            logger.info("Estendendo previs√£o usando regress√£o linear nos valores da previs√£o original")
            
            # Calcular a tend√™ncia (inclina√ß√£o) usando regress√£o linear
            x = np.arange(len(forecast_values))
            slope, intercept = np.polyfit(x, forecast_values, 1)
            
            # Calcular a variabilidade (desvio padr√£o) presente nos valores de previs√£o
            std_dev = np.std(forecast_values)
            
            # Definir o ru√≠do como uma fra√ß√£o do desvio padr√£o ou um valor m√≠nimo
            noise_level = max(std_dev * 0.5, abs(np.mean(forecast_values)) * 0.02) if np.mean(forecast_values) != 0 else 0.1
            
            # Gerar novos valores baseados na tend√™ncia com um pouco de ru√≠do
            extended_values = forecast_values.copy()
            last_value = forecast_values[-1]
            
            for i in range(len(forecast_values), len(future_dates)):
                # Calcular o pr√≥ximo valor projetado pela tend√™ncia
                next_value = last_value + slope
                
                # Adicionar um pouco de ru√≠do aleat√≥rio para evitar que a linha seja perfeitamente reta
                noise = np.random.normal(0, noise_level)
                next_value += noise
                
                extended_values.append(next_value)
                last_value = next_value
                
            return extended_values
        
        # Quando temos valores iguais ou quase iguais, ou apenas um valor de previs√£o
        elif len(unique_values) <= 2 or len(forecast_values) == 1:
            logger.info("Valores de previs√£o s√£o iguais ou quase iguais. Usando dados hist√≥ricos para estimar tend√™ncia.")
            
            # Se temos dados hist√≥ricos dispon√≠veis
            if df is not None and len(df) > 5:
                # Calcular tend√™ncia dos dados hist√≥ricos
                hist_values = df['y'].values
                hist_x = np.arange(len(hist_values))
                
                # Usar regress√£o linear para estimar tend√™ncia
                hist_slope, _ = np.polyfit(hist_x, hist_values, 1)
                
                # Calcular variabilidade hist√≥rica (desvio padr√£o)
                hist_std = np.std(hist_values)
                
                # Definir o ru√≠do como uma fra√ß√£o do desvio padr√£o ou um valor m√≠nimo
                noise_level = max(hist_std * 0.5, abs(np.mean(hist_values)) * 0.02) if np.mean(hist_values) != 0 else 0.1
                
                # Estender os valores usando a tend√™ncia hist√≥rica
                extended_values = forecast_values.copy()
                last_value = forecast_values[-1]
                
                for i in range(len(forecast_values), len(future_dates)):
                    # Calcular o pr√≥ximo valor baseado na tend√™ncia hist√≥rica 
                    next_value = last_value + hist_slope
                    
                    # Adicionar ru√≠do para criar variabilidade natural
                    noise = np.random.normal(0, noise_level)
                    next_value += noise
                    
                    extended_values.append(next_value)
                    last_value = next_value
                    
                return extended_values
                
            else:
                # Sem dados hist√≥ricos, usar uma abordagem mais simples com variabilidade
                logger.info("Sem dados hist√≥ricos dispon√≠veis. Usando o √∫ltimo valor da previs√£o com variabilidade.")
                
                extended_values = forecast_values.copy()
                last_value = forecast_values[-1]
                
                # Definir um n√≠vel de ru√≠do proporcional ao valor
                base_noise = abs(last_value) * 0.03 if last_value != 0 else 0.1
                
                for i in range(len(forecast_values), len(future_dates)):
                    # Adicionar uma pequena varia√ß√£o aleat√≥ria ao √∫ltimo valor
                    noise = np.random.normal(0, base_noise)
                    next_value = last_value + noise
                    
                    extended_values.append(next_value)
                    last_value = next_value
                
                return extended_values 